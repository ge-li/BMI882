# 50 Years of Data Science

### [Donoho (2017)](https://doi.org/10.1080/10618600.2017.1384734)

##### Title: 50 Years of Data Science

##### Author: David Donoho

In this paper, the author reviews the 50 years of data science since the publication of John Tukey's "Future of Data Analysis" (FoDA, Tukey 1962). There are hypes and many recent new degree programs in Data Science. The author compares Data Science v. Statistics by including opinions drastically different from saying *Data Science is Statistics* to *Statistics is irrelevant* to Data Science, and some memes. The author then reviews the FoDA paper and 50 years of exhortations and reification of data science as a concept and field. The author also uses a section to introduce Breiman’s “Two Cultures” paper (Breiman 2001), where "generative modeling" and "algorithmic modeling" are discussed. The author argues that the Common Task Framework (CTF) is the secret sauce of the success of predictive modeling culture (i.e. algorithmic modeling). The author then uses the curriculum of UC Berkley's new DS master program as an example to discuss the current problem with teaching data science, in order to introduce the 6 divisions of greater data science (GDS) in the next section. The author further talks about GDS6 - Science of Data Science - in greater detail, by mentioning cross-study and cross-workflow analysis as examples. In the end, the author looks into the next 50 years' development of Data Science. 

All in all, it's a very enjoyable and comprehensive paper to read. It's a good summary of what we learned from BMI 881.

* The memes are so true. I strongly agree with the author's opinion on distributed databases, that many of the techniques involved will be trivialized. 
* About the CTF and evaluation under the empirical success criteria, there's an old Chinese saying that "it doesn’t matter whether it is a white cat or a black cat, as long as it catches mice, it is a good cat". 
* GDS should include open access and reproducibility, especially when it comes to "it is not surprising to be involved in ambitious projects using a half dozen languages in concert". But then I realized that the author includes the discussion of open science in section 10. 

Questions

* Can we really do good data analysis without a deep understanding of corresponding domain knowledge in mind? I ask this because the author talks a lot about the data science (or analysis) as a program or subject, and some general guild lines. A hypothetical situation is that could a student graduated from, say, the UC Berkley's DS master program, with the course listed, be a good survival data analyst? The problem is that I don't think they're real experts in Data Science in general. They can only be experts either some specific types of data or some aspects of data analysis. 
* Further, Do we need to create an all-round data scientist mastering all GDS framework? We can have people collaborate. A collaboration of experts is better than a cohort of know-how amateurs.
* Simple methods are as good as more complex methods. --> Decision boundary. Most of our modern ML methods can reach maximum performance on common datasets. Question: say if we exploit our dataset, how do we find other predictors that are best based on our current knowledge?
