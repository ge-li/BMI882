# Citation Statistics

### [Adler et al. (2009)](https://doi.org/10.1214/09-STS285)

##### Title: Citation Statistics

##### Authors: Robert Adler, John Ewing and Peter Taylor

"Publish or perish." It seems that the publication records of scientists tie strongly with their "success." Evaluating scientists' research productivity and quality by their publication is certainly justifiable. The question arises when we ask how to do it properly. With the advancement of information technology and the growth of publication and citation data, the idea of using "simple and objective" methods, i.e., bibliometrics, to assess scientific research is becoming increasingly popular. This article discusses the use and misuse of citation data and statistics, such as impact factor, h-index, etc. The authors argue that although citation statistics should not be ignored entirely, resorting solely to them is misleading and illusory, that we should use citation statistics wisely and with cautionary.

I'm impressed by the Transactions of AMS vs. Proceedings example, where the authors prove there is little sense to compare individual papers by their impact factor. The most apparent drawback I can see if we evaluate someone's work only by citation statistics is that such a system can be easily manipulated and exploited. There is a joke about self-citation that goes, "I published nine papers, and I have 45 citations." 

Questions:
1. How should we assess the research quality? It seems unanimous that we shouldn't rely solely on citation statistics. Then is there any practicable scheme other than total subjectivity?
2. What's the current practice of research assessment in our department? 